{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d172beb",
   "metadata": {},
   "source": [
    "# NLP Practice Assignments\n",
    "Day 1\n",
    "\n",
    "1. What is the purpose of text preprocessing in NLP, and why is it essential before analysis?\n",
    "\n",
    "2. Describe tokenization in NLP and explain its significance in text processing.\n",
    "\n",
    "3. What are the differences between stemming and lemmatization in NLP? When would you \n",
    "choose one over the other?\n",
    "\n",
    "4. Explain the concept of stop words and their role in text preprocessing. How do they impact \n",
    "NLP tasks?\n",
    "\n",
    "5. How does the process of removing punctuation contribute to text preprocessing in NLP? \n",
    "What are its benefits?\n",
    "\n",
    "6. Discuss the importance of lowercase conversion in text preprocessing. Why is it a \n",
    "common step in NLP tasks?\n",
    "\n",
    "7. Explain the term \"vectorization\" concerning text data. How does techniques like \n",
    "CountVectorizer contribute to text preprocessing in NLP?\n",
    "\n",
    "8. Describe the concept of normalization in NLP. Provide examples of normalization \n",
    "techniques used in text preprocessing.\n",
    "\n",
    "\n",
    "Note: Consider the text. It may be a file or prompted inputs.\n",
    " Python code is mandate for possible Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc011b3e",
   "metadata": {},
   "source": [
    "# 1. Purpose of Text Preprocessing in NLP:\n",
    "    \n",
    "Text preprocessing in NLP serves the purpose of cleaning and organizing raw text data to make it suitable for analysis. It involves various techniques like tokenization, stemming, lemmatization, and removal of irrelevant information. Preprocessing is essential to:\n",
    "\n",
    "Enhance the efficiency of analysis algorithms.\n",
    "Reduce the dimensionality of the data.\n",
    "Improve the accuracy and interpretability of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebe4578",
   "metadata": {},
   "source": [
    "# 2. Tokenization in NLP:\n",
    "Tokenization is the process of breaking text into words, phrases, symbols, or other meaningful elements (tokens). It is a crucial step in text processing as it helps in:\n",
    "\n",
    "Understanding the structure of sentences.\n",
    "Facilitating further analysis by converting text into manageable units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e985967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'a', 'key', 'step', 'in', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Tokenization is a key step in NLP.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e863fe",
   "metadata": {},
   "source": [
    "# 3. Stemming vs. Lemmatization:\n",
    "Stemming and lemmatization are techniques to reduce words to their base or root form. \n",
    "Stemming is faster but less accurate, while lemmatization considers context and is more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9b9efca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed words: ['stem', 'and', 'lemmat', 'are', 'techniqu', '.']\n",
      "Lemmatized words: ['Stemming', 'and', 'lemmatization', 'are', 'technique', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "words = word_tokenize(\"Stemming and lemmatization are techniques.\")\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "print(\"Stemmed words:\", stemmed_words)\n",
    "print(\"Lemmatized words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61083f2e",
   "metadata": {},
   "source": [
    "# 4 Stop Words:\n",
    "Stop words are common words like \"the,\" \"is,\" and \"and\" that are often removed during text preprocessing. \n",
    "They don't contribute much to the meaning of the text but may impact analysis efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b48f26c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered tokens: ['Stop', 'words', 'removed', 'better', 'analysis', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Stop words should be removed for better analysis.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_tokens = [word for word in word_tokenize(text) if word.lower() not in stop_words]\n",
    "print(\"Filtered tokens:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc23764",
   "metadata": {},
   "source": [
    "# 5. Removing Punctuation:\n",
    "Removing punctuation helps in focusing on the actual words and their meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea21b906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text without punctuation: Removing punctuation Does it improve text processing\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "text = \"Removing punctuation: Does it improve text processing?\"\n",
    "no_punctuations = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "print(\"Text without punctuation:\", no_punctuations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b149d403",
   "metadata": {},
   "source": [
    "# 6. Lowercase Conversion:\n",
    "Lowercasing is essential for consistency and uniformity in text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afa9628a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercased text: convert this text to lowercase.\n"
     ]
    }
   ],
   "source": [
    "text = \"Convert This Text To Lowercase.\"\n",
    "lowercased_text = text.lower()\n",
    "print(\"Lowercased text:\", lowercased_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed586195",
   "metadata": {},
   "source": [
    "# 7. Vectorization:\n",
    "Vectorization involves converting text data into numerical vectors. \n",
    "CountVectorizer is a common technique for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fca2fd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['document' 'first' 'is' 'second' 'the' 'this']\n",
      "Token Counts Matrix: [[1 1 1 0 1 1]\n",
      " [2 0 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "documents = [\"This is the first document.\", \"This document is the second document.\"]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "print(\"Feature names:\", vectorizer.get_feature_names_out())\n",
    "print(\"Token Counts Matrix:\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c1c12b",
   "metadata": {},
   "source": [
    "# 8. Normalization in NLP:\n",
    "Normalization involves transforming text data to a standard form. Common techniques include stemming, lemmatization, and lowercasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00c5e865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized words: ['normal', 'techniqu', 'includ', 'stem', 'and', 'lemmat', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Normalization techniques include stemming and lemmatization.\"\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "normalized_words = [stemmer.stem(word) for word in word_tokenize(text.lower())]\n",
    "print(\"Normalized words:\", normalized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7133ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
